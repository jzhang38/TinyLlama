<div align="center">

# TinyLlama-1.1B
[English](README.md) | ä¸­æ–‡

[Chat Demo](https://huggingface.co/spaces/TinyLlama/tinyllama-chat)
</div>

TinyLlamaé¡¹ç›®æ—¨åœ¨åœ¨3ä¸‡äº¿tokensä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ„å»ºä¸€ä¸ªæ‹¥æœ‰11äº¿å‚æ•°çš„Llamaæ¨¡å‹ã€‚ç»è¿‡ç²¾å¿ƒä¼˜åŒ–ï¼Œæˆ‘ä»¬"ä»…"éœ€16å—A100-40Gçš„GPUï¼Œä¾¿å¯åœ¨90å¤©å†…å®Œæˆè¿™ä¸ªä»»åŠ¡ğŸš€ğŸš€ã€‚è®­ç»ƒå·²äº2023-09-01å¼€å§‹ã€‚


<div align="center">
  <img src=".github/TinyLlama_logo.png" width="300"/>
</div>
æˆ‘ä»¬é‡‡ç”¨äº†ä¸Llama 2å®Œå…¨ç›¸åŒçš„æ¶æ„å’Œåˆ†è¯å™¨ã€‚è¿™æ„å‘³ç€TinyLlamaå¯ä»¥åœ¨è®¸å¤šåŸºäºLlamaçš„å¼€æºé¡¹ç›®ä¸­å³æ’å³ç”¨ã€‚æ­¤å¤–ï¼ŒTinyLlamaåªæœ‰1.1Bçš„å‚æ•°ï¼Œä½“ç§¯å°å·§ï¼Œé€‚ç”¨äºéœ€è¦é™åˆ¶è®¡ç®—å’Œå†…å­˜å ç”¨çš„å¤šç§åº”ç”¨ã€‚

#### æ–°é—»

* 2023-12-18ï¼š
  * æ·»åŠ ä¸¤ä¸ªæ–‡æ¡£ [1](https://whimsical-aphid-86d.notion.site/Release-of-TinyLlama-1-5T-Checkpoints-Postponed-01b266998c1c47f78f5ae1520196d194?pvs=4), [2](https://whimsical-aphid-86d.notion.site/Latest-Updates-from-TinyLlama-Team-7d30c01fff794da28ccc952f327c8d4f?pvs=4) è¯´æ˜è®­ç»ƒæ›²çº¿ã€é¡¹ç›®æ—¶é—´è¡¨å’Œé”™è¯¯ä¿®å¤çš„å˜åŒ–ã€‚
* 2023-10-03: 
  * åœ¨speculative decodingä¸­æ·»åŠ llama.cppçš„ä»£ç ç¤ºä¾‹ã€‚å…·ä½“è¯·æŸ¥çœ‹ [speculative_decoding/README.md](speculative_decoding/README.md)ã€‚
  * 2023-10-02: 1. 1T-tokenæ£€æŸ¥ç‚¹åˆšå‘å¸ƒã€‚2. æˆ‘ä»¬åœ¨[huggingface](https://huggingface.co/TinyLlama/tinyLlama-intermediate-checkpoints/tree/step-480k-token-1007B)ä¸Šè®°å½•äº†**æ‰€æœ‰**ä¸­é—´æ£€æŸ¥ç‚¹ã€‚
  * 2023-09-28: å¯ç”¨[Discord](https://discord.gg/74Wcx4j5Nb)æœåŠ¡å™¨ã€‚
* 2023-09-18: 
  * å‘å¸ƒäº†ä¸€ä¸ª [chat demo](https://huggingface.co/spaces/TinyLlama/tinyllama-chat)ï¼Œæ¬¢è¿ç‚¹å‡»é“¾æ¥æ¥å°è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚
* 2023-09-16: 
  * å‘å¸ƒäº†ç›®å‰å·²ç»è®­ç»ƒäº† 5.03 äº¿ä¸ª token çš„ [checkpoints æ¨¡å‹](https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b)ã€‚ 
  * åŸºäº 5.03 äº¿ token çš„ [checkpoints æ¨¡å‹](https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b) åœ¨ OpenAssistant æ•°æ®é›†ä¸Šå¾®è°ƒå¹¶å¼€æºäº†èŠå¤©æ¨¡å‹ [TinyLlama-Chat-V0.1](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.1) ï¼Œå¹¶æ·»åŠ äº†æˆ‘ä»¬çš„ [å¾®è°ƒè„šæœ¬](sft) ã€‚
  * æ·»åŠ äº†æ›´å¤šçš„è¯„æµ‹æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥é€šè¿‡ [EVAL.md](EVAL.md) æ–‡ä»¶æ¥æŸ¥çœ‹æˆ‘ä»¬å„æ¨¡å‹çš„ç»“æœã€‚




#### å‘å¸ƒæ—¶é—´è¡¨

æˆ‘ä»¬ä¼šæ ¹æ®ä»¥ä¸‹è®¡åˆ’é€æ­¥å‘å¸ƒä¸­é—´checkpointã€‚æˆ‘ä»¬ä¹Ÿåˆ—äº†ä¸€äº›åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚

åŸºåº§æ¨¡å‹:

| Date       | æ¨¡å‹æƒé‡                                              | Tokens | Step | Commonsense Avg |
| ---------- | ------------------------------------------------------------ | ------ | ---- | --------------- |
| 2023-09-01 | Pythia-1.0B                                                  | 300B   | 143k | 48.30           |
| 2023-09-04 | [TinyLlama-1.1B-intermediate-step-50k-105b](https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b) ([ModelScope](https://www.modelscope.cn/models/chaoscodes/TinyLlama-1.1B-step-50K-105b/files)) | 105B   | 50k  | 46.11           |
| 2023-09-16 | [TinyLlama-1.1B-intermediate-step-240k-503b](https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-240k-503b) ([ModelScope](https://www.modelscope.cn/models/chaoscodes/TinyLlama-1.1B-intermediate-step-240k-503b/files)) | 503B   | 240K | 48.28           |
| 2023-10-01 | [TinyLlama-1.1B-intermediate-step-480k-1T](https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-480k-1T) | 1T     | 480k | 50.22 |
| 2023-11-04 | [TinyLlama-1.1B-intermediate-step-715k-1.5T](https://huggingface.co/PY007/TinyLlama-1.1B-intermediate-step-715k-1.5T)                                            | 1.5T     |715k    |51.28 |
| 2023-11-20 | [TinyLlama-1.1B-intermediate-step-955k-2T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-955k-token-2T)                                            | 2T     |955k    |51.64 |
| 2023-12-11 | [TinyLlama-1.1B-intermediate-step-1195k-2.5T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1195k-token-2.5T)              | 2.5T     | 1195k    |53.86 |
| 2023-12-28 | [TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T)              | 3T   | 1431k  | 52.99 |

å¯¹è¯æ¨¡å‹:

| Date       | æ¨¡å‹æƒé‡                                  | Tokens | Step | Commonsense Avg |
|------------|-------------------------------------------------|--------|------| --------------- |
| 2023-09-16 | [TinyLlama-1.1B-Chat-V0.1](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.1) ([ModelScope](https://www.modelscope.cn/models/chaoscodes/TinyLlama-1.1B-Chat-v0.1/files))                                         | 503B   | 240K    |  49.57 |
| 2023-10-1 | [TinyLlama-1.1B-Chat-V0.3](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.3)                                            | 1T   | 480K    |  51.36 |
| 2023-11-04 | [TinyLlama-1.1B-Chat-V0.4](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4)                                            | 1.5T   | 715K    |  52.30 |

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬çš„ç°åœ¨æ¨¡å‹è¿˜å¤„äºè®­ç»ƒåˆæœŸï¼Œå­¦ä¹ ç‡å¹¶æ²¡æœ‰å®Œå…¨ç¨³å®šä¸‹æ¥ï¼Œä¸ºäº†æ›´å¥½çš„ä½“éªŒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä¸‹è½½æˆ‘ä»¬ [èŠå¤©æ¨¡å‹](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) æˆ–è€…é€šè¿‡ [chat demo](https://huggingface.co/spaces/TinyLlama/tinyllama-chat) æ¥å°è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚


ä½ ä»¬ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://api.wandb.ai/links/lance777/pgvhrsny)å®æ—¶è·Ÿè¸ªTinyLlamaçš„è®­ç»ƒæŸå¤±ã€‚

## æ½œåœ¨åœºæ™¯
å°å‹ä½†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹å¯¹è®¸å¤šåº”ç”¨éƒ½å¾ˆæœ‰ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ½œåœ¨çš„åœºæ™¯ï¼š
- å¸®åŠ©å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œspeculative decodingã€‚
- åœ¨è¾¹ç¼˜è£…ç½®ä¸Šè¿è¡Œï¼Œæ¯”å¦‚ç¦»çº¿çš„å®æ—¶æœºå™¨ç¿»è¯‘ (TinyLlamaçš„4æ¯”ç‰¹é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹æƒé‡åªéœ€è¦550MBçš„å†…å­˜)ã€‚
- åœ¨æ¸¸æˆä¸­å®ç°å®æ—¶å¯¹è¯ç”Ÿæˆ(å› ä¸ºè¿˜å¾—ç»™æ¸¸æˆæœ¬èº«ç•™æ˜¾å­˜æ‰€ä»¥æ¨¡å‹è¦å°)ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»£ç å¯ä»¥ç»™åˆå­¦è€…åšä¸€ä¸ª**å…¥é—¨é¢„è®­ç»ƒçš„ç®€æ´å‚è€ƒ**ã€‚å¦‚æœä½ è¦è®­ç»ƒ50äº¿ä»¥ä¸‹å‚æ•°çš„è¯­è¨€æ¨¡å‹, ä½ å…¶å®ä¸éœ€è¦Megatron-LMã€‚

## è®­ç»ƒç»†èŠ‚
ä»¥ä¸‹æ˜¯æˆ‘ä»¬è®­ç»ƒè®¾ç½®çš„ä¸€äº›ç»†èŠ‚ï¼š

| Setting                         | Description                                                    |
|---------------------------------|----------------------------------------------------------------|
| Parameters                      | 1.1B                                                           |
| Attention Variant               | Grouped Query Attention                                        |
| Model Size                      | Layers: 22, Heads: 32, Query Groups: 4, Embedding Size: 2048, Intermediate Size (Swiglu): 5632|
| Sequence Length                 | 2048                                                           |
| Batch Size                      | 2 million tokens (2048 * 1024)                                             |
| Learning Rate                   | 4e-4                                                           |
| Learning Rate Schedule          | Cosine with 2000 warmup steps                                  |
| Training Data                   | [Slimpajama](https://huggingface.co/datasets/cerebras/slimpajama-627b) & [Starcoderdata](https://huggingface.co/datasets/bigcode/starcoderdata) |
| Data Preprocessing              | Excluded GitHub subset of Slimpajama; Sampled all code from Starcoderdata |
| Combined Dataset Size           | Around 950B tokens                                              |
| Total Tokens During Training    | 3 trillion (slightly more than 3 epochs/143k steps)                                          |
| Natural Language to Code Ratio  | 7:3                                                            |
| Hardware                        | 16 A100-40G GPUs                                               |






## é€Ÿåº¦æå¿«
æˆ‘ä»¬çš„ä»£ç åº“æ”¯æŒä»¥ä¸‹ç‰¹æ€§ï¼š
- ä½¿ç”¨FSDPè¿›è¡Œå¤šGPUå’Œå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼è®­ç»ƒ
- flash attention 2
- èåˆå±‚å½’ä¸€åŒ– (fused layernorm)
- èåˆswiglu (fused swiglu)
- èåˆäº¤å‰ç†µæŸå¤± (fused cross entropy loss)
- èåˆæ—‹è½¬ä½ç½®åµŒå…¥ (fused rotary positional embedding)

è‡´è°¢ï¼šflash attention 2ã€èåˆå±‚å½’ä¸€åŒ–ã€èåˆäº¤å‰ç†µæŸå¤±å’Œèåˆæ—‹è½¬ä½ç½®åµŒå…¥æ¥è‡ªäº[FlashAttention](https://github.com/Dao-AILab/flash-attention/)ä»“åº“ï¼›èåˆswigluæ¥è‡ªäº[xformers](https://github.com/facebookresearch/xformers)ã€‚

æœ‰äº†è¿™äº›ä¼˜åŒ–, æˆ‘ä»¬å¯ä»¥è¾¾åˆ°**24k tokens/ç§’/A100**çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¹Ÿå°±æ˜¯56%çš„MFUï¼ˆåœ¨A100-80Gä¸Šçš„MFUä¼šæ›´é«˜ï¼‰ã€‚è¿™ä¸ªé€Ÿåº¦å¯ä»¥è®©ä½ å¯ä»¥åœ¨**8ä¸ªA100ä¸Šç”¨32å°æ—¶è®­ç»ƒä¸€ä¸ªchinchilla-optimialçš„æ¨¡å‹**(11äº¿å‚æ•°ï¼Œ220äº¿token)ã€‚è¿™äº›ä¼˜åŒ–ä¹Ÿå¤§å¤§å‡å°‘äº†æ˜¾å­˜å ç”¨, æˆ‘ä»¬å¯ä»¥æŠŠ11äº¿å‚æ•°çš„æ¨¡å‹å¡å…¥40GBçš„GPUé‡Œé¢è¿˜èƒ½åŒæ—¶ç»´æŒ16k tokensçš„per-gpu batch sizeã€‚åªéœ€è¦æŠŠbatch sizeæ”¹å°ä¸€ç‚¹ï¼Œ ä½ å°±å¯ä»¥åœ¨**RTX 3090/4090**ä¸Šé¢è®­ç»ƒTinyLlamaã€‚
ä¸‹é¢æ˜¯æˆ‘ä»¬çš„ä»£ç åº“ä¸Pythiaå’ŒMPTçš„è®­ç»ƒé€Ÿåº¦çš„æ¯”è¾ƒã€‚


| Model                             | A100 GPU hours taken on 300B tokens| 
|-----------------------------------|------------------------------------|
|TinyLlama-1.1B                     | 3456                               |    
|[Pythia-1.0B](https://huggingface.co/EleutherAI/pythia-1b)                        | 4830                               |
|[MPT-1.3B](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)                           | 7920                               |  

<small> Pythiaçš„æ•°å­—æ¥è‡ªä»–ä»¬çš„è®ºæ–‡ã€‚MPTçš„æ•°å­—æ¥è‡ª[è¿™é‡Œ](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)ï¼Œä½œè€…è¯´MPT-1.3B"was trained on 440 A100-40GBs for about half a day" on 200B tokensã€‚</small>

TinyLlamaæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„æ¨¡å‹, åŒæ—¶æˆ‘ä»¬ç”¨äº†GQA, è¿™æ„å‘³ç€å®ƒåœ¨æ¨ç†æœŸé—´ä¹Ÿå¾ˆå¿«ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬æµ‹é‡çš„ä¸€äº›æ¨ç†é€Ÿåº¦ï¼š

| Framework | Device | Settings | Throughput (tokens/sec) |
|-----------|--------------|-----|-----------|
|[Llama.cpp](https://github.com/ggerganov/llama.cpp) | Mac M2 16GB RAM         |  batch_size=1; 4-bit inference|    71.8     | 
|[vLLM](https://github.com/vllm-project/vllm)       | A40 GPU  | batch_size=100, n=10 |   7094.5         |


## å¼€å§‹é¢„è®­ç»ƒ
è¯·å‚è€ƒ[PRETRAIN.md](PRETRAIN.md)ã€‚



## å¾®è°ƒ

* æˆ‘ä»¬åœ¨ [sft](sft) ä¸­æ·»åŠ äº†æˆ‘ä»¬è¿›è¡Œå¾®è°ƒå’Œæ¨ç†çš„ä»£ç ã€‚å¹¶ä¸”åŸºäºè¿™ä¸ªä»£ç æˆ‘ä»¬åœ¨[openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¾—åˆ°äº†æˆ‘ä»¬çš„ç¬¬ä¸€ç‰ˆ[èŠå¤©æ¨¡å‹](https://huggingface.co/PY007/TinyLlama-1.1B-Chat-v0.1)ã€‚
* å¦‚æœæ‚¨å¸Œæœ›åœ¨ RAM å°äº 4GB çš„ GPU ä¸Šå¯¹ç”¨æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¯ä»¥å‚è€ƒå¹¶ä½¿ç”¨ [Qlora](https://github.com/artidoro/qlora) å’Œ [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) é¡¹ç›®ã€‚
* ç›®å‰å¾®è°ƒçš„æ—¶å€™æˆ‘ä»¬å¹¶æ²¡æœ‰å¹¿æ³›å¯¹è¶…å‚è¿›è¡Œæœç´¢ï¼Œä¹Ÿæ²¡æœ‰é€‰æ‹©æ½œåœ¨æ›´ä¼˜çš„ instruction æ•°æ®é›†ã€‚æˆ‘ä»¬å¸Œæœ›ä¿ƒè¿› NLP ç¤¾åŒºå¯¹äºæˆ‘ä»¬çš„TinyLlamaæ¨¡å‹çš„å¼€æ”¾ç ”ç©¶ï¼Œå¹¶å¼€æºæ›´å¥½çš„å¾®è°ƒèŠå¤©æ¨¡å‹ã€‚æˆ‘ä»¬ä¹Ÿä¼šæŠŠè¿™äº›æ¨¡å‹æ”¾åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ã€‚



## TODO
è¯¥é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ã€‚æˆ‘ä»¬å›¢é˜Ÿå¾ˆå°ï¼Œéå¸¸æ¬¢è¿ç¤¾åŒºçš„åé¦ˆå’Œè´¡çŒ®ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬è®¡åˆ’è¿›è¡Œçš„ä¸€äº›å·¥ä½œï¼š
 - [ ] Add scripts for pretraining on other datasets.
 - [ ] Sequence length extrapolation.
 - [ ] Test out speculative decoding for Llama-2-7B.
 - [ ] Test the throughput on RTX 3090/4090. 
 - [ ] Add fine-tuning scripts.
 - [ ] Properly evaluate the model on downstream tasks.
 - [ ] A demo running on mobile phones. 
 - [ ] Explore retrieval-augmentation.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=jzhang38/TinyLlama&type=Date)](https://star-history.com/#jzhang38/TinyLlama&Date)


## Acknowledgements
è¿™ä¸ªä»“åº“åŸºäºå‡ºè‰²çš„å¼€æºé¡¹ç›®[lit-gpt](https://github.com/Lightning-AI/lit-gpt)å’Œ[flash-attention](https://github.com/Dao-AILab/flash-attention)æ„å»º. 
```
@online{lit-gpt,
  author    = {Lightning AI},
  title     = {Lit-GPT},
  url       = {https://github.com/Lightning-AI/lit-gpt},
  year      = {2023},
}
@article{dao2023flashattention2,
  title     ={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author    ={Dao, Tri},
  year      ={2023}
}
```

## Citation
æ­¤é¡¹ç›®ç›®å‰ç”±[Peiyuan Zhang](https://github.com/jzhang38)ï¼Œ[Guangtao Zeng](https://github.com/ChaosCodes)ï¼Œ[Tianduo Wang](https://github.com/TianduoWang)å’Œ[Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei/)è´¡çŒ®ã€‚ 

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œ å¯ä»¥å¼•ç”¨:

```
@misc{zhang2024tinyllama,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```

