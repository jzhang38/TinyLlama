<div align="center">

# TinyLlama-1.1B
[English](README.md) | ä¸­æ–‡
</div>

TinyLlamaé¡¹ç›®æ—¨åœ¨åœ¨30ä¸‡äº¿tokensä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ„å»ºä¸€ä¸ªæ‹¥æœ‰11äº¿å‚æ•°çš„Llamaæ¨¡å‹ã€‚ç»è¿‡ç²¾å¿ƒä¼˜åŒ–ï¼Œæˆ‘ä»¬"ä»…"éœ€16å—A100-40Gçš„GPUï¼Œä¾¿å¯åœ¨90å¤©å†…å®Œæˆè¿™ä¸ªä»»åŠ¡ğŸš€ğŸš€ã€‚è®­ç»ƒå·²äº2023-09-01å¼€å§‹ã€‚


<div align="center">
  <img src=".github/TinyLlama_logo.png" width="300"/>
</div>

æˆ‘ä»¬é‡‡ç”¨äº†ä¸Llama 2å®Œå…¨ç›¸åŒçš„æ¶æ„å’Œåˆ†è¯å™¨ã€‚è¿™æ„å‘³ç€TinyLlamaå¯ä»¥åœ¨è®¸å¤šåŸºäºLlamaçš„å¼€æºé¡¹ç›®ä¸­å³æ’å³ç”¨ã€‚æ­¤å¤–ï¼ŒTinyLlamaåªæœ‰1.1Bçš„å‚æ•°ï¼Œä½“ç§¯å°å·§ï¼Œé€‚ç”¨äºéœ€è¦é™åˆ¶è®¡ç®—å’Œå†…å­˜å ç”¨çš„å¤šç§åº”ç”¨ã€‚


#### å‘å¸ƒæ—¶é—´è¡¨

æˆ‘ä»¬ä¼šæ ¹æ®ä»¥ä¸‹è®¡åˆ’é€æ­¥å‘å¸ƒä¸­é—´checkpointã€‚æˆ‘ä»¬ä¹Ÿåˆ—äº†ä¸€äº›åŸºçº¿æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚



| Date       | HF Checkpoint                                   | Tokens | Step | HellaSwag Acc_norm |
|------------|-------------------------------------------------|--------|------|---------------------|
| Baseline   | [StableLM-Alpha-3B](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)| 800B   | --   |  38.31            |
| Baseline   | [Pythia-1B-intermediate-step-50k-105b](https://huggingface.co/EleutherAI/pythia-1b/tree/step50000)             | 105B   | 50k   |  42.04            |
| Baseline   | [Pythia-1B](https://huggingface.co/EleutherAI/pythia-1b)             | 300B   | 143k   |  47.16            |
| 2023-09-04 | [TinyLlama-1.1B-intermediate-step-50k-105b](https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b) | 105B   | 50k   |  43.50               |
| 2023-09-16 | --                                             | 500B   | --   |  --               |
| 2023-10-01 | --                                             | 1T     | --   |  --               |
| 2023-10-16 | --                                             | 1.5T   | --   |  --               |
| 2023-10-31 | --                                             | 2T     | --   |  --               |
| 2023-11-15 | --                                             | 2.5T   | --   |  --               |
| 2023-12-01 | --                                             | 3T     | --   |  --               |

<!-- | Baseline   | [Pythia-1B-intermediate-52b](https://huggingface.co/EleutherAI/pythia-1b/tree/step25000)             | 52B   | 25k   |  38.81            | -->
<!-- | Baseline   | [Pythia-1.4B-intermediate-52b](https://huggingface.co/EleutherAI/pythia-1.4b/tree/step25000)             | 52B   | 25k   |  42.49            | -->
<!-- | Baseline   | [Pythia-1.4B-intermediate-105b](https://huggingface.co/EleutherAI/pythia-1.4b/tree/step50000)             | 105B   | 50k   |  46.14            | -->

ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼ŒTinyLlamaç›®å‰çš„è¿›å±•éå¸¸å¥½ğŸ‰ğŸ‰ã€‚


ä½ ä¹Ÿå¯ä»¥åœ¨[è¿™é‡Œ](https://wandb.ai/lance777/lightning_logs/reports/metric-train_loss-23-09-02-15-26-17---Vmlldzo1MjkzNzMw?accessToken=9843chbl7rfi1w03hxttpcnbo9z8t6088pw3ddn4h8teunaq0cy7j8hw9c5i02ve)å®æ—¶è·Ÿè¸ªTinyLlamaçš„è®­ç»ƒæŸå¤±ã€‚

## æ½œåœ¨åœºæ™¯
å°å‹ä½†å¼ºå¤§çš„è¯­è¨€æ¨¡å‹å¯¹è®¸å¤šåº”ç”¨éƒ½å¾ˆæœ‰ç”¨ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›æ½œåœ¨çš„åœºæ™¯ï¼š
- å¸®åŠ©å¯¹å¤§å‹æ¨¡å‹è¿›è¡Œspeculative decodingã€‚
- åœ¨è¾¹ç¼˜è£…ç½®ä¸Šè¿è¡Œï¼Œæ¯”å¦‚ç¦»çº¿çš„å®æ—¶æœºå™¨ç¿»è¯‘ (TinyLlamaçš„4æ¯”ç‰¹é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹æƒé‡åªéœ€è¦550MBçš„å†…å­˜)ã€‚
- åœ¨æ¸¸æˆä¸­å®ç°å®æ—¶å¯¹è¯ç”Ÿæˆ(å› ä¸ºè¿˜å¾—ç»™æ¸¸æˆæœ¬èº«ç•™æ˜¾å­˜æ‰€ä»¥æ¨¡å‹è¦å°)ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ä»£ç å¯ä»¥ç»™åˆå­¦è€…åšä¸€ä¸ª**å…¥é—¨é¢„è®­ç»ƒçš„ç®€æ´å‚è€ƒ**ã€‚å¦‚æœä½ è¦è®­ç»ƒ50äº¿ä»¥ä¸‹å‚æ•°çš„è¯­è¨€æ¨¡å‹, ä½ å…¶å®ä¸éœ€è¦Megatron-LMã€‚

## è®­ç»ƒç»†èŠ‚
ä»¥ä¸‹æ˜¯æˆ‘ä»¬è®­ç»ƒè®¾ç½®çš„ä¸€äº›ç»†èŠ‚ï¼š

| Setting                         | Description                                                    |
|---------------------------------|----------------------------------------------------------------|
| Parameters                      | 1.1B                                                           |
| Attention Variant               | Grouped Query Attention                                        |
| Model Size                      | Layers: 22, Heads: 32, Query Groups: 4, Embedding Size: 2048, Intermediate Size (Swiglu): 5632|
| Sequence Length                 | 2048                                                           |
| Batch Size                      | 2 million tokens (2048 * 1024)                                             |
| Learning Rate                   | 4e-4                                                           |
| Learning Rate Schedule          | Cosine with 2000 warmup steps                                  |
| Training Data                   | [Slimpajama](https://huggingface.co/datasets/cerebras/slimpajama-627b) & [Starcoderdata](https://huggingface.co/datasets/bigcode/starcoderdata) |
| Data Preprocessing              | Excluded GitHub subset of Slimpajama; Sampled all code from Starcoderdata |
| Combined Dataset Size           | 1 trillion tokens                                              |
| Total Tokens During Training    | 3 trillion (3 epochs/143k steps)                                          |
| Natural Language to Code Ratio  | 7:3                                                            |
| Hardware                        | 16 A100-40G GPUs                                               |






## é€Ÿåº¦æå¿«
æˆ‘ä»¬çš„ä»£ç åº“æ”¯æŒä»¥ä¸‹ç‰¹æ€§ï¼š
- multi-gpu and multi-node distributed training with FSDP.
- flash attention 2.
- fused layernorm.
- fused swiglu.
- fused cross entropy loss .
- fused rotary positional embedding.

æœ‰äº†è¿™äº›ä¼˜åŒ–, æˆ‘ä»¬å¯ä»¥è¾¾åˆ°**24k tokens/ç§’/A100**çš„è®­ç»ƒé€Ÿåº¦ï¼Œä¹Ÿå°±æ˜¯56%çš„MFUï¼ˆåœ¨A100-80Gä¸Šçš„MFUä¼šæ›´é«˜ï¼‰ã€‚è¿™ä¸ªé€Ÿåº¦å¯ä»¥è®©ä½ å¯ä»¥åœ¨**8ä¸ªA100ä¸Šç”¨32å°æ—¶è®­ç»ƒä¸€ä¸ªchinchilla-optimialçš„æ¨¡å‹**(11äº¿å‚æ•°ï¼Œ220äº¿token)ã€‚è¿™äº›ä¼˜åŒ–ä¹Ÿå¤§å¤§å‡å°‘äº†æ˜¾å­˜å ç”¨, æˆ‘ä»¬å¯ä»¥æŠŠ11äº¿å‚æ•°çš„æ¨¡å‹å¡å…¥40GBçš„GPUé‡Œé¢è¿˜èƒ½åŒæ—¶ç»´æŒ16k tokensçš„per-gpu batch sizeã€‚åªéœ€è¦æŠŠbatch sizeæ”¹å°ä¸€ç‚¹ï¼Œ ä½ å°±å¯ä»¥åœ¨**RTX 3090/4090**ä¸Šé¢è®­ç»ƒTinyLlamaã€‚
ä¸‹é¢æ˜¯æˆ‘ä»¬çš„ä»£ç åº“ä¸Pythiaå’ŒMPTçš„è®­ç»ƒé€Ÿåº¦çš„æ¯”è¾ƒã€‚


| Model                             | A100 GPU hours taken on 300B tokens| 
|-----------------------------------|------------------------------------|
|TinyLlama-1.1B                     | 3456                               |    
|[Pythia-1.0B](https://huggingface.co/EleutherAI/pythia-1b)                        | 4830                               |
|[MPT-1.3B](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)                           | 7920                               |  

<small> Pythiaçš„æ•°å­—æ¥è‡ªä»–ä»¬çš„è®ºæ–‡ã€‚MPTçš„æ•°å­—æ¥è‡ª[è¿™é‡Œ](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)ï¼Œä½œè€…è¯´MPT-1.3B"was trained on 440 A100-40GBs for about half a day" on 200B tokensã€‚</small>

TinyLlamaæ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„æ¨¡å‹, åŒæ—¶æˆ‘ä»¬ç”¨äº†GQA, è¿™æ„å‘³ç€å®ƒåœ¨æ¨ç†æœŸé—´ä¹Ÿå¾ˆå¿«ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬æµ‹é‡çš„ä¸€äº›æ¨ç†é€Ÿåº¦ï¼š

| Framework | Device | Batch Size | Throughput (tokens/sec) |
|-----------|--------------|-----|-----------|
|[Llama.cpp](https://github.com/ggerganov/llama.cpp) | Mac M2 16GB RAM         |  batch_size=1; 4-bit inference|    71.8     | 
|[vLLM](https://github.com/vllm-project/vllm)       | A40 GPU  | batch_size=100, n=10 |   7094.5         |


## å¼€å§‹è®­ç»ƒ
è¯·å‚è€ƒ[PRETRAIN.md](PRETRAIN.md)ã€‚

## TODO
è¯¥é¡¹ç›®ä»åœ¨ç§¯æå¼€å‘ä¸­ã€‚æˆ‘ä»¬å›¢é˜Ÿå¾ˆå°ï¼Œéå¸¸æ¬¢è¿ç¤¾åŒºçš„åé¦ˆå’Œè´¡çŒ®ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬è®¡åˆ’è¿›è¡Œçš„ä¸€äº›å·¥ä½œï¼š
 - [ ] Add scripts for pretraining on other datasets.
 - [ ] Sequence length extrapolation.
 - [ ] Test the throughput on RTX 3090/4090. 
 - [ ] Add fine-tuning scripts.
 - [ ] Properly evaluate the model on downstream tasks.
 - [ ] A demo running on mobile phones. 
 - [ ] Explore retrieval-augmentation.


## Acknowledgements
è¿™ä¸ªä»“åº“åŸºäºå‡ºè‰²çš„å¼€æºé¡¹ç›®[lit-gpt](https://github.com/Lightning-AI/lit-gpt)å’Œ[flash-attention](https://github.com/Dao-AILab/flash-attention)æ„å»º. 
```
@online{lit-gpt,
  author    = {Lightning AI},
  title     = {Lit-GPT},
  url       = {https://github.com/Lightning-AI/lit-gpt},
  year      = {2023},
}
@article{dao2023flashattention2,
  title     ={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author    ={Dao, Tri},
  year      ={2023}
}
```

## Citation
æ­¤é¡¹ç›®ç›®å‰ç”±[Peiyuan Zhang](https://github.com/jzhang38)ï¼Œ[Guangtao Zeng](https://github.com/ChaosCodes)ï¼Œ[Tianduo Wang](https://github.com/TianduoWang)å’Œ[Wei Lu](https://istd.sutd.edu.sg/people/faculty/lu-wei/)è´¡çŒ®ã€‚ 

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰ä»·å€¼ï¼Œ å¯ä»¥å¼•ç”¨:

```
@online{tinyllama,
  author    = {Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu},
  title     = {TinyLlama},
  url       = {https://github.com/jzhang38/TinyLlama},
  year      = {2023},
  month     = {Sep},
}
```
